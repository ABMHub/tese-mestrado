@article{formUnderstandingSurvey,
  author       = {Abdelrahman Abdallah and
                  Daniel Eberharter and
                  Zoe Pfister and
                  Adam Jatowt},
  title        = {A survey of recent approaches to form understanding in scanned documents},
  journal      = {Artificial Intelligence Review},
  volume       = {57},
  number       = {12},
  year         = {2024},
  url          = {https://doi.org/10.1007/s10462-024-11000-0},
  doi          = {10.1007/s10462-024-11000-0}
}

@inproceedings{documentNet,
  author       = {Lijun Yu and
                  Jin Miao and
                  Xiaoyu Sun and
                  Jiayi Chen and
                  Alexander G. Hauptmann and
                  Hanjun Dai and
                  Wei Wei},
  editor       = {Mingxuan Wang and
                  Imed Zitouni},
  title        = {DocumentNet: Bridging the Data Gap in Document Pre-training},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing: {EMNLP} 2023 - Industry Track, Singapore, December
                  6-10, 2023},
  pages        = {707--722},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-industry.66},
  doi          = {10.18653/V1/2023.EMNLP-INDUSTRY.66},
  timestamp    = {Sun, 04 Aug 2024 19:38:43 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/YuMSCHD023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhong2019publaynet,
  author       = {Xu Zhong and
                  Jianbin Tang and
                  Antonio Jimeno{-}Yepes},
  title        = {PubLayNet: Largest Dataset Ever for Document Layout Analysis},
  booktitle    = {2019 International Conference on Document Analysis and Recognition,
                  {ICDAR} 2019, Sydney, Australia, September 20-25, 2019},
  pages        = {1015--1022},
  publisher    = {{IEEE}},
  year         = {2019},
  url          = {https://doi.org/10.1109/ICDAR.2019.00166},
  doi          = {10.1109/ICDAR.2019.00166},
  timestamp    = {Mon, 15 Jun 2020 17:05:49 +0200},
  biburl       = {https://dblp.org/rec/conf/icdar/ZhongTJ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{harley2015rvlcdip,
  author       = {Adam W. Harley and
                  Alex Ufkes and
                  Konstantinos G. Derpanis},
  title        = {Evaluation of deep convolutional nets for document image classification
                  and retrieval},
  booktitle    = {13th International Conference on Document Analysis and Recognition,
                  {ICDAR} 2015, Nancy, France, August 23-26, 2015},
  pages        = {991--995},
  publisher    = {{IEEE} Computer Society},
  year         = {2015},
  url          = {https://doi.org/10.1109/ICDAR.2015.7333910},
  doi          = {10.1109/ICDAR.2015.7333910},
  timestamp    = {Fri, 24 Mar 2023 00:05:06 +0100},
  biburl       = {https://dblp.org/rec/conf/icdar/HarleyUD15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{vrdMatchingPaper,
  author       = {Ritesh Sarkhel and
                  Arnab Nandi},
  title        = {Cross-Modal Entity Matching for Visually Rich Documents},
  journal      = {CoRR},
  volume       = {abs/2303.00720},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.00720},
  doi          = {10.48550/ARXIV.2303.00720},
  eprinttype    = {arXiv},
  eprint       = {2303.00720},
  timestamp    = {Mon, 06 Mar 2023 16:51:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-00720.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{fslSurvey2022,
  author       = {Yisheng Song and
                  Ting Wang and
                  Puyu Cai and
                  Subrota K. Mondal and
                  Jyoti Prakash Sahoo},
  title        = {A Comprehensive Survey of Few-shot Learning: Evolution, Applications,
                  Challenges, and Opportunities},
  journal      = {{ACM} Comput. Surv.},
  volume       = {55},
  number       = {13s},
  pages        = {271:1--271:40},
  year         = {2023},
  url          = {https://doi.org/10.1145/3582688},
  doi          = {10.1145/3582688},
  timestamp    = {Wed, 23 Oct 2024 16:44:12 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/Song0CMS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mathew2021docvqa,
  author       = {Minesh Mathew and
                  Dimosthenis Karatzas and
                  C. V. Jawahar},
  title        = {DocVQA: {A} Dataset for {VQA} on Document Images},
  booktitle    = {{IEEE} Winter Conference on Applications of Computer Vision, {WACV}
                  2021, Waikoloa, HI, USA, January 3-8, 2021},
  pages        = {2199--2208},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/WACV48630.2021.00225},
  doi          = {10.1109/WACV48630.2021.00225},
  timestamp    = {Mon, 03 Mar 2025 21:24:07 +0100},
  biburl       = {https://dblp.org/rec/conf/wacv/MathewKJ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jaume2019funsd,
  author       = {Guillaume Jaume and
                  Hazim Kemal Ekenel and
                  Jean{-}Philippe Thiran},
  title        = {{FUNSD:} {A} Dataset for Form Understanding in Noisy Scanned Documents},
  booktitle    = {2nd International Workshop on Open Services and Tools for Document
                  Analysis, OST@ICDAR 2019, Sydney, Australia, September 22-25, 2019},
  pages        = {1--6},
  publisher    = {{IEEE}},
  year         = {2019},
  url          = {https://doi.org/10.1109/ICDARW.2019.10029},
  doi          = {10.1109/ICDARW.2019.10029},
  timestamp    = {Thu, 14 Oct 2021 10:14:47 +0200},
  biburl       = {https://dblp.org/rec/conf/icdar/JaumeET19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{iitcdip2006,
  author       = {David D. Lewis and
                  Gady Agam and
                  Shlomo Argamon and
                  Ophir Frieder and
                  David A. Grossman and
                  Jefferson Heard},
  editor       = {Efthimis N. Efthimiadis and
                  Susan T. Dumais and
                  David Hawking and
                  Kalervo J{\"{a}}rvelin},
  title        = {Building a test collection for complex document information processing},
  booktitle    = {{SIGIR} 2006: Proceedings of the 29th Annual International {ACM} {SIGIR}
                  Conference on Research and Development in Information Retrieval, Seattle,
                  Washington, USA, August 6-11, 2006},
  pages        = {665--666},
  publisher    = {{ACM}},
  year         = {2006},
  url          = {https://doi.org/10.1145/1148170.1148307},
  doi          = {10.1145/1148170.1148307},
  timestamp    = {Wed, 14 Nov 2018 10:58:10 +0100},
  biburl       = {https://dblp.org/rec/conf/sigir/LewisAAFGH06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhou2022document,
  author       = {Ejian Zhou and
                  Xingjiao Wu and
                  Luwei Xiao and
                  Xiangcheng Du and
                  Tianlong Ma and
                  Liang He},
  title        = {Document Layout Analysis Via Positional Encoding},
  booktitle    = {2022 {IEEE} International Conference on Image Processing, {ICIP} 2022,
                  Bordeaux, France, 16-19 October 2022},
  pages        = {1156--1160},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/ICIP46576.2022.9897330},
  doi          = {10.1109/ICIP46576.2022.9897330},
  timestamp    = {Sun, 12 Nov 2023 02:07:13 +0100},
  biburl       = {https://dblp.org/rec/conf/icip/ZhouWXDM022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{EER3,
  author={Agrawal, Pinki and Kapoor, Ravikant and Agrawal, Sanjay},
  booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
  title={A hybrid partial fingerprint matching algorithm for estimation of Equal error rate}, 
  year={2014},
  volume={},
  number={},
  pages={1295-1299},
  keywords={Fingerprint recognition;Physiology;Guidelines;Transforms;Image segmentation;Silicon;Partial fingerprint;Minutiae extraction;Radon transform;Pores features;Local Binary Pattern;Score level fusion;FAR;FRR},
  doi={10.1109/ICACCCT.2014.7019308}
}

@INPROCEEDINGS{EER2,
  author={Hofbauer, Heinz and Uhl, Andreas},
  booktitle={2016 International Conference on Biometrics (ICB)}, 
  title={Calculating a boundary for the significance from the equal-error rate}, 
  year={2016},
  volume={},
  number={},
  pages={1-4},
  keywords={Databases;Estimation;Iris recognition;Computers;Partitioning algorithms;Data mining;Error analysis},
  doi={10.1109/ICB.2016.7550053}
}



@inproceedings{EER,
  title={ICDAR 2021 competition on on-line signature verification},
  author={Tolosana, Ruben and Vera-Rodriguez, Ruben and Gonzalez-Garcia, Carlos and Fierrez, Julian and Rengifo, Santiago and Morales, Aythami and Ortega-Garcia, Javier and Carlos Ruiz-Garcia, Juan and Romero-Tapiador, Sergio and Jiang, Jiajia and others},
  booktitle={Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part IV 16},
  pages={723--737},
  year={2021},
  organization={Springer}
}

@inproceedings{kim2024text,
  author       = {Hye Jin Kim and
                  Nicolas Lell and
                  Ansgar Scherp},
  editor       = {Amon Rapp and
                  Luigi Di Caro and
                  Farid Meziane and
                  Vijayan Sugumaran},
  title        = {Text Role Classification in Scientific Charts Using Multimodal Transformers},
  booktitle    = {Natural Language Processing and Information Systems - 29th International
                  Conference on Applications of Natural Language to Information Systems,
                  {NLDB} 2024, Turin, Italy, June 25-27, 2024, Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14762},
  pages        = {47--61},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-70239-6\_4},
  doi          = {10.1007/978-3-031-70239-6\_4},
  timestamp    = {Tue, 22 Oct 2024 21:07:54 +0200},
  biburl       = {https://dblp.org/rec/conf/nldb/KimLS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sevim2022document,
author="Sevim, Semih
and Omurca, Sevin{\c{c}} {\.{I}}lhan
and Ekinci, Ekin",
editor="Seyman, Muhammet Nuri",
title="Document Image Classification with Vision Transformers",
booktitle="Electrical and Computer Engineering",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="68--81",
isbn="978-3-031-01984-5"
}


@inproceedings{scius2024zeroshot,
  author       = {Anna Scius{-}Bertrand and
                  Michael Jungo and
                  Lars V{\"{o}}gtlin and
                  Jean{-}Marc Spat and
                  Andreas Fischer},
  editor       = {Apostolos Antonacopoulos and
                  Subhasis Chaudhuri and
                  Rama Chellappa and
                  Cheng{-}Lin Liu and
                  Saumik Bhattacharya and
                  Umapada Pal},
  title        = {Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document
                  Image Classification Using Large Language Models},
  booktitle    = {Pattern Recognition - 27th International Conference, {ICPR} 2024,
                  Kolkata, India, December 1-5, 2024, Proceedings, Part {XIX}},
  series       = {Lecture Notes in Computer Science},
  volume       = {15319},
  pages        = {152--166},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-78495-8\_10},
  doi          = {10.1007/978-3-031-78495-8\_10},
  timestamp    = {Thu, 12 Dec 2024 11:11:52 +0100},
  biburl       = {https://dblp.org/rec/conf/icpr/SciusBertrandJVSF24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{veneri2022document,
  author       = {Ali Youssef and
                  Gabriele Valvano and
                  Giacomo Veneri},
  editor       = {Michelangelo Ceci and
                  Sergio Flesca and
                  Elio Masciari and
                  Giuseppe Manco and
                  Zbigniew W. Ras},
  title        = {Document Layout Analysis with Variational Autoencoders: An Industrial
                  Application},
  booktitle    = {Foundations of Intelligent Systems - 26th International Symposium,
                  {ISMIS} 2022, Cosenza, Italy, October 3-5, 2022, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {13515},
  pages        = {477--486},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-3-031-16564-1\_46},
  doi          = {10.1007/978-3-031-16564-1\_46},
  timestamp    = {Fri, 11 Oct 2024 07:47:20 +0200},
  biburl       = {https://dblp.org/rec/conf/ismis/YoussefVV22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{kothandaraman2023salad,
  author       = {Divya Kothandaraman and
                  Sumit Shekhar and
                  Abhilasha Sancheti and
                  Manoj Ghuhan and
                  Tripti Shukla and
                  Dinesh Manocha},
  title        = {{SALAD} : Source-free Active Label-Agnostic Domain Adaptation for
                  Classification, Segmentation and Detection},
  booktitle    = {{IEEE/CVF} Winter Conference on Applications of Computer Vision, {WACV}
                  2023, Waikoloa, HI, USA, January 2-7, 2023},
  pages        = {382--391},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/WACV56688.2023.00046},
  doi          = {10.1109/WACV56688.2023.00046},
  timestamp    = {Tue, 14 Feb 2023 22:26:55 +0100},
  biburl       = {https://dblp.org/rec/conf/wacv/KothandaramanSSGSM23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{sinha2024cica,
  author       = {Sankalp Sinha and
                  Muhammad Saif Ullah Khan and
                  Talha Uddin Sheikh and
                  Didier Stricker and
                  Muhammad Zeshan Afzal},
  editor       = {Elisa H. Barney Smith and
                  Marcus Liwicki and
                  Liangrui Peng},
  title        = {{CICA:} Content-Injected Contrastive Alignment for Zero-Shot Document
                  Image Classification},
  booktitle    = {Document Analysis and Recognition - {ICDAR} 2024 - 18th International
                  Conference, Athens, Greece, August 30 - September 4, 2024, Proceedings,
                  Part {IV}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14807},
  pages        = {124--141},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-70546-5\_8},
  doi          = {10.1007/978-3-031-70546-5\_8},
  timestamp    = {Thu, 03 Oct 2024 00:45:06 +0200},
  biburl       = {https://dblp.org/rec/conf/icdar/SinhaKSSA24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{van2024distildoc,
  author       = {Jordy Van Landeghem and
                  Subhajit Maity and
                  Ayan Banerjee and
                  Matthew B. Blaschko and
                  Marie{-}Francine Moens and
                  Josep Llad{\'{o}}s and
                  Sanket Biswas},
  editor       = {Elisa H. Barney Smith and
                  Marcus Liwicki and
                  Liangrui Peng},
  title        = {DistilDoc: Knowledge Distillation for Visually-Rich Document Applications},
  booktitle    = {Document Analysis and Recognition - {ICDAR} 2024 - 18th International
                  Conference, Athens, Greece, August 30 - September 4, 2024, Proceedings,
                  Part {IV}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14807},
  pages        = {195--217},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-70546-5\_12},
  doi          = {10.1007/978-3-031-70546-5\_12},
  timestamp    = {Fri, 20 Sep 2024 14:01:09 +0200},
  biburl       = {https://dblp.org/rec/conf/icdar/LandeghemMBBMLB24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zeghidi2023cdpsim,
  author       = {H{\'{e}}di Zeghidi and
                  Carlos Fernando Crispim Junior and
                  Iuliia Tkachenko},
  title        = {CDP-Sim: Similarity Metric Learning to Identify the Fake Copy Detection
                  Patterns},
  booktitle    = {{IEEE} International Workshop on Information Forensics and Security,
                  {WIFS} 2023, N{\"{u}}rnberg, Germany, December 4-7, 2023},
  pages        = {1--6},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/WIFS58808.2023.10374744},
  doi          = {10.1109/WIFS58808.2023.10374744},
  timestamp    = {Wed, 17 Jan 2024 09:05:03 +0100},
  biburl       = {https://dblp.org/rec/conf/wifs/ZeghidiCT23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{li2021fsparn,
  author       = {Yujie Li and
                  Pengfei Zhang and
                  Xing Xu and
                  Yi Lai and
                  Fumin Shen and
                  Lijiang Chen and
                  Pengxiang Gao},
  title        = {Few-shot prototype alignment regularization network for document image
                  layout segementation},
  journal      = {Pattern Recognit.},
  volume       = {115},
  pages        = {107882},
  year         = {2021},
  url          = {https://doi.org/10.1016/j.patcog.2021.107882},
  doi          = {10.1016/J.PATCOG.2021.107882},
  timestamp    = {Fri, 23 Apr 2021 09:00:36 +0200},
  biburl       = {https://dblp.org/rec/journals/pr/LiZXLSCG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jingnan2023igsc,
  author       = {Jingnan Li and
                  Chuan Lin and
                  Ruizhang Huang and
                  Yongbin Qin and
                  Yanping Chen},
  title        = {Intention-guided deep semi-supervised document clustering via metric
                  learning},
  journal      = {J. King Saud Univ. Comput. Inf. Sci.},
  volume       = {35},
  number       = {1},
  pages        = {416--425},
  year         = {2023},
  url          = {https://doi.org/10.1016/j.jksuci.2022.12.010},
  doi          = {10.1016/J.JKSUCI.2022.12.010},
  timestamp    = {Sat, 13 May 2023 01:06:26 +0200},
  biburl       = {https://dblp.org/rec/journals/jksucis/LiLHQC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhu2023layout,
  author       = {Maosheng Zhu and
                  Ruijie Ni},
  title        = {Layout Similarity Based Key Information Extraction Framework for Structural Images},
  booktitle    = {Proceedings of the 2023 International Conference on Image Processing, Computer Vision and Machine Learning ({ICICML})},
  pages        = {181--184},
  year         = {2023},
  doi          = {10.1109/ICICML60161.2023.10424746},
  url          = {https://doi.org/10.1109/ICICML60161.2023.10424746},
  timestamp    = {Fri, 07 Mar 2025 12:00:00 +0100}
}

@article{ward1963,
  author       = {Joe H. Ward Jr.},
  title        = {Hierarchical Grouping to Optimize an Objective Function},
  journal      = {Journal of the American Statistical Association},
  volume       = {58},
  number       = {301},
  pages        = {236--244},
  year         = {1963},
  doi          = {10.1080/01621459.1963.10500845},
  url          = {https://doi.org/10.1080/01621459.1963.10500845}
}


@inproceedings{gzsl,
  author       = {Yongqin Xian and
                  Tobias Lorenz and
                  Bernt Schiele and
                  Zeynep Akata},
  title        = {Feature Generating Networks for Zero-Shot Learning},
  booktitle    = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages        = {5542--5551},
  publisher    = {Computer Vision Foundation / {IEEE} Computer Society},
  year         = {2018},
  url          = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Xian\_Feature\_Generating\_Networks\_CVPR\_2018\_paper.html},
  doi          = {10.1109/CVPR.2018.00581},
  timestamp    = {Sat, 30 Sep 2023 09:38:24 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/XianLSA18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{internvl2024,
  author       = {Zhe Chen and
                  Weiyun Wang and
                  Yue Cao and
                  Yangzhou Liu and
                  Zhangwei Gao and
                  Erfei Cui and
                  Jinguo Zhu and
                  Shenglong Ye and
                  Hao Tian and
                  Zhaoyang Liu and
                  Lixin Gu and
                  Xuehui Wang and
                  Qingyun Li and
                  Yimin Ren and
                  Zixuan Chen and
                  Jiapeng Luo and
                  Jiahao Wang and
                  Tan Jiang and
                  Bo Wang and
                  Conghui He and
                  Botian Shi and
                  Xingcheng Zhang and
                  Han Lv and
                  Yi Wang and
                  Wenqi Shao and
                  Pei Chu and
                  Zhongying Tu and
                  Tong He and
                  Zhiyong Wu and
                  Huipeng Deng and
                  Jiaye Ge and
                  Kai Chen and
                  Min Dou and
                  Lewei Lu and
                  Xizhou Zhu and
                  Tong Lu and
                  Dahua Lin and
                  Yu Qiao and
                  Jifeng Dai and
                  Wenhai Wang},
  title        = {Expanding Performance Boundaries of Open-Source Multimodal Models
                  with Model, Data, and Test-Time Scaling},
  journal      = {CoRR},
  volume       = {abs/2412.05271},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.05271},
  doi          = {10.48550/ARXIV.2412.05271},
  eprinttype    = {arXiv},
  eprint       = {2412.05271},
  timestamp    = {Mon, 27 Jan 2025 08:08:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-05271.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{openai2024,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@article{sgd,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}


@misc{adamw,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cosinelr,
  author       = {Zhao Liu},
  title        = {Super Convergence Cosine Annealing with Warm-Up Learning Rate},
  booktitle    = {{CAIBDA} 2022, 2nd International Conference on Artificial Intelligence,
                  Big Data and Algorithms, Nanjing, China, 17-19 June 2022},
  pages        = {1--7},
  publisher    = {{VDE} / {IEEE}},
  year         = {2022},
  url          = {https://ieeexplore.ieee.org/document/10104453},
  timestamp    = {Wed, 06 Dec 2023 14:35:25 +0100},
  biburl       = {https://dblp.org/rec/conf/caibda/Liu22c.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{imagenet,
  author       = {Jia Deng and
                  Wei Dong and
                  Richard Socher and
                  Li{-}Jia Li and
                  Kai Li and
                  Li Fei{-}Fei},
  title        = {ImageNet: {A} large-scale hierarchical image database},
  booktitle    = {2009 {IEEE} Computer Society Conference on Computer Vision and Pattern
                  Recognition {(CVPR} 2009), 20-25 June 2009, Miami, Florida, {USA}},
  pages        = {248--255},
  publisher    = {{IEEE} Computer Society},
  year         = {2009},
  url          = {https://doi.org/10.1109/CVPR.2009.5206848},
  doi          = {10.1109/CVPR.2009.5206848},
  timestamp    = {Fri, 08 Nov 2024 10:13:55 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/DengDSLL009.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{kfoldcv,
  author       = {Tzu{-}Tsung Wong and
                  Po{-}Yang Yeh},
  title        = {Reliable Accuracy Estimates from k-Fold Cross Validation},
  journal      = {{IEEE} Trans. Knowl. Data Eng.},
  volume       = {32},
  number       = {8},
  pages        = {1586--1594},
  year         = {2020},
  url          = {https://doi.org/10.1109/TKDE.2019.2912815},
  doi          = {10.1109/TKDE.2019.2912815},
  timestamp    = {Thu, 06 Aug 2020 21:46:18 +0200},
  biburl       = {https://dblp.org/rec/journals/tkde/WongY20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{funsd2019,
  author    = {Guillaume Jaume and
               Haz{\i}m Kemal Ekenel and
               Jean{-}Philippe Thiran},
  title     = {{FUNSD:} {A} Dataset for Form Understanding in Noisy Scanned Documents},
  booktitle = {2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},
  pages     = {1--6},
  year      = {2019},
  doi       = {10.1109/ICDARW.2019.00009}
}

@inproceedings{huang_icdar2019,
  author       = {Zheng Huang and
                  Kai Chen and
                  Jianhua He and
                  Xiang Bai and
                  Dimosthenis Karatzas and
                  Shijian Lu and
                  C. V. Jawahar},
  title        = {{ICDAR2019} Competition on Scanned Receipt {OCR} and Information Extraction},
  booktitle    = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
  pages        = {1516--1520},
  year         = {2019},
  month        = {September},
  publisher    = {IEEE},
  doi          = {10.1109/ICDAR.2019.00244},
  url          = {https://doi.org/10.1109/ICDAR.2019.00244}
}


@inproceedings{cord2019,
  author    = {Seunghyun Park and
               Seung Shin and
               Byeongchang Kim and
               Junbum Cha and
               Hwalsuk Lee},
  title     = {{CORD:} A Consolidated Receipt Dataset for Post-OCR Parsing},
  booktitle = {Document Intelligence Workshop at NeurIPS 2019},
  year      = {2019},
  url       = {https://arxiv.org/abs/1908.07414}
}

@article{xfund2021,
  author       = {Yiheng Xu and
                  Tengchao Lv and
                  Lei Cui and
                  Guoxin Wang and
                  Yijuan Lu and
                  Dinei Flor{\^{e}}ncio and
                  Cha Zhang and
                  Furu Wei},
  title        = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich
                  Document Understanding},
  journal      = {CoRR},
  volume       = {abs/2104.08836},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.08836},
  eprinttype    = {arXiv},
  eprint       = {2104.08836},
  timestamp    = {Thu, 14 Oct 2021 09:17:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-08836.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{meta2024llama3.2,
  title     = {Llama 3.2: From Cloud to Edge, Now with Vision},
  author    = {Meta AI},
  howpublished = {\url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}},
  year      = {2024}
}

@misc{openai2024gpt4o,
  title={Hello GPT-4o},
  author={OpenAI},
  howpublished={\url{https://openai.com/index/hello-gpt-4o/}},
  year={2024}
}

@misc{openai2024gpt4omini,
  title={GPT-4o mini: advancing cost-efficient intelligence},
  author={OpenAI},
  howpublished={\url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}},
  year={2024}
}


@article{liu_document_2021,
  author       = {Li Liu and
                  Zhiyu Wang and
                  Taorong Qiu and
                  Qiu Chen and
                  Yue Lu and
                  Ching Y. Suen},
  title        = {Document image classification: Progress over two decades},
  journal      = {Neurocomputing},
  volume       = {453},
  pages        = {223--240},
  year         = {2021},
  url          = {https://doi.org/10.1016/j.neucom.2021.04.114},
  doi          = {10.1016/J.NEUCOM.2021.04.114},
  timestamp    = {Wed, 24 Aug 2022 08:14:05 +0200},
  biburl       = {https://dblp.org/rec/journals/ijon/LiuWQCLS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{doctr2021,
    title={docTR: Document Text Recognition},
    author={Mindee},
    year={2021},
    publisher = {GitHub},
    howpublished = {\url{https://github.com/mindee/doctr}}
}


@article{kay_tesseract_2007,
  title={Tesseract: an open-source optical character recognition engine},
  author={Kay, Anthony},
  journal={Linux Journal},
  volume={2007},
  number={159},
  pages={2},
  year={2007},
  publisher={Belltown Media Houston, TX}
}

@article{xian_zero-shot_2019,
  author       = {Yongqin Xian and
                  Christoph H. Lampert and
                  Bernt Schiele and
                  Zeynep Akata},
  title        = {Zero-Shot Learning - {A} Comprehensive Evaluation of the Good, the
                  Bad and the Ugly},
  journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  volume       = {41},
  number       = {9},
  pages        = {2251--2265},
  year         = {2019},
  url          = {https://doi.org/10.1109/TPAMI.2018.2857768},
  doi          = {10.1109/TPAMI.2018.2857768},
  timestamp    = {Sat, 30 Sep 2023 10:23:26 +0200},
  biburl       = {https://dblp.org/rec/journals/pami/XianLSA19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{he_deep_2016,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  booktitle    = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages        = {770--778},
  publisher    = {{IEEE} Computer Society},
  year         = {2016},
  url          = {https://doi.org/10.1109/CVPR.2016.90},
  doi          = {10.1109/CVPR.2016.90},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tan_efficientnet_2019,
  author       = {Mingxing Tan and
                  Quoc V. Le},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {6105--6114},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/tan19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/TanL19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{howard_searching_2019,
  author       = {Andrew Howard and
                  Ruoming Pang and
                  Hartwig Adam and
                  Quoc V. Le and
                  Mark Sandler and
                  Bo Chen and
                  Weijun Wang and
                  Liang{-}Chieh Chen and
                  Mingxing Tan and
                  Grace Chu and
                  Vijay Vasudevan and
                  Yukun Zhu},
  title        = {Searching for MobileNetV3},
  booktitle    = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
                  2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages        = {1314--1324},
  publisher    = {{IEEE}},
  year         = {2019},
  url          = {https://doi.org/10.1109/ICCV.2019.00140},
  doi          = {10.1109/ICCV.2019.00140},
  timestamp    = {Thu, 27 May 2021 16:20:51 +0200},
  biburl       = {https://dblp.org/rec/conf/iccv/HowardPALSCWCTC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{simonyan_very_2015,
  author       = {Karen Simonyan and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.1556},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dosovitskiy_image_2021,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{khosla_supervised_2021,
  author       = {Zhiqian Zhao and
                  Yinghou Jiao and
                  Yeyin Xu and
                  Runchao Zhao},
  title        = {A supervised contrastive learning method based on online complement
                  strategy for long-tailed fine-grained fault diagnosis},
  journal      = {Adv. Eng. Informatics},
  volume       = {64},
  pages        = {103079},
  year         = {2025},
  url          = {https://doi.org/10.1016/j.aei.2024.103079},
  doi          = {10.1016/J.AEI.2024.103079},
  timestamp    = {Mon, 03 Mar 2025 21:26:20 +0100},
  biburl       = {https://dblp.org/rec/journals/aei/ZhaoJXZ25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{chopra_learning_2005,
  author       = {Sumit Chopra and
                  Raia Hadsell and
                  Yann LeCun},
  title        = {Learning a Similarity Metric Discriminatively, with Application to
                  Face Verification},
  booktitle    = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern
                  Recognition {(CVPR} 2005), 20-26 June 2005, San Diego, CA, {USA}},
  pages        = {539--546},
  publisher    = {{IEEE} Computer Society},
  year         = {2005},
  url          = {https://doi.org/10.1109/CVPR.2005.202},
  doi          = {10.1109/CVPR.2005.202},
  timestamp    = {Fri, 28 Feb 2025 14:40:17 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/ChopraHL05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/eacl/JhaSRCR23,
  author       = {Akshita Jha and
                  Adithya Samavedhi and
                  Vineeth Rakesh and
                  Jaideep Chandrashekar and
                  Chandan K. Reddy},
  editor       = {Andreas Vlachos and
                  Isabelle Augenstein},
  title        = {Transformer-based Models for Long-Form Document Matching: Challenges
                  and Empirical Analysis},
  booktitle    = {Findings of the Association for Computational Linguistics: {EACL}
                  2023, Dubrovnik, Croatia, May 2-6, 2023},
  pages        = {2300--2310},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-eacl.178},
  doi          = {10.18653/V1/2023.FINDINGS-EACL.178},
  timestamp    = {Thu, 05 Oct 2023 18:04:59 +0200},
  biburl       = {https://dblp.org/rec/conf/eacl/JhaSRCR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@incollection{chicco_siamese_2021,
	address = {New York, NY},
	title = {Siamese {Neural} {Networks}: {An} {Overview}},
	isbn = {978-1-0716-0826-5},
	shorttitle = {Siamese {Neural} {Networks}},
	url = {https://doi.org/10.1007/978-1-0716-0826-5_3},
	abstract = {Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman’s rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.},
	language = {en},
	urldate = {2025-03-08},
	booktitle = {Artificial {Neural} {Networks}},
	publisher = {Springer US},
	author = {Chicco, Davide},
	editor = {Cartwright, Hugh},
	year = {2021},
	doi = {10.1007/978-1-0716-0826-5_3},
	pages = {73--94},
}

@article{DBLP:journals/corr/abs-2412-10302,
  author       = {Zhiyu Wu and
                  Xiaokang Chen and
                  Zizheng Pan and
                  Xingchao Liu and
                  Wen Liu and
                  Damai Dai and
                  Huazuo Gao and
                  Yiyang Ma and
                  Chengyue Wu and
                  Bingxuan Wang and
                  Zhenda Xie and
                  Yu Wu and
                  Kai Hu and
                  Jiawei Wang and
                  Yaofeng Sun and
                  Yukun Li and
                  Yishi Piao and
                  Kang Guan and
                  Aixin Liu and
                  Xin Xie and
                  Yuxiang You and
                  Kai Dong and
                  Xingkai Yu and
                  Haowei Zhang and
                  Liang Zhao and
                  Yisong Wang and
                  Chong Ruan},
  title        = {DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced
                  Multimodal Understanding},
  journal      = {CoRR},
  volume       = {abs/2412.10302},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.10302},
  doi          = {10.48550/ARXIV.2412.10302},
  eprinttype    = {arXiv},
  eprint       = {2412.10302},
  timestamp    = {Thu, 06 Mar 2025 08:12:24 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-10302.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bai2025qwen2,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2502.13923}
}

@misc{googlecolab,
  author = {Google Research},
  title = {Google Colaboratory},
  year = {2024},
  howpublished = "\url{https://colab.research.google.com/}"
}

@inproceedings{huang_sroie_2019,
	title = {{SROIE}: {A} benchmark dataset for scanned receipts {OCR} and information extraction},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Huang, Z. and Jin, L. and Liu, X.},
	year = {2019},
	pages = {993--997},
}

@article{bakkali_eaml_2021,
	title = {{EAML}: ensemble self-attention-based mutual learning network for document image classification},
	volume = {24},
	issn = {1433-2833},
	shorttitle = {{EAML}},
	url = {https://doi.org/10.1007/s10032-021-00378-0},
	doi = {10.1007/s10032-021-00378-0},
	abstract = {In the recent past, complex deep neural networks have received huge interest in various document understanding tasks such as document image classification and document retrieval. As many document types have a distinct visual style, learning only visual features with deep CNNs to classify document images has encountered the problem of low inter-class discrimination, and high intra-class structural variations between its categories. In parallel, text-level understanding jointly learned with the corresponding visual properties within a given document image has considerably improved the classification performance in terms of accuracy. In this paper, we design a self-attention-based fusion module that serves as a block in our ensemble trainable network. It allows to simultaneously learn the discriminant features of image and text modalities throughout the training stage. Besides, we encourage mutual learning by transferring the positive knowledge between image and text modalities during the training stage. This constraint is realized by adding a truncated Kullback–Leibler divergence loss (Tr-KLDReg) as a new regularization term, to the conventional supervised setting. To the best of our knowledge, this is the first time to leverage a mutual learning approach along with a self-attention-based fusion module to perform document image classification. The experimental results illustrate the effectiveness of our approach in terms of accuracy for the single-modal and multi-modal modalities. Thus, the proposed ensemble self-attention-based mutual learning model outperforms the state-of-the-art classification results based on the benchmark RVL-CDIP and Tobacco-3482 datasets.},
	number = {3},
	urldate = {2025-06-03},
	journal = {Int. J. Doc. Anal. Recognit.},
	author = {Bakkali, Souhail and Ming, Zuheng and Coustaty, Mickaël and Rusiñol, Marçal},
	month = sep,
	year = {2021},
	pages = {251--268},
}


@inproceedings{yang_batchsampler_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {{BatchSampler}: {Sampling} {Mini}-{Batches} for {Contrastive} {Learning} in {Vision}, {Language}, and {Graphs}},
	isbn = {979-8-4007-0103-0},
	shorttitle = {{BatchSampler}},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599263},
	doi = {10.1145/3580305.3599263},
	abstract = {In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives within the current mini-batch, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler{\textbackslash}footnoteThe code is available at BatchSampler to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sample hard-to-distinguish instances. BatchSampler is a simple and general technique that can be directly plugged into existing contrastive learning models in vision, language, and graphs. Extensive experiments on datasets of three modalities show that BatchSampler can consistently improve the performance of powerful contrastive models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE on STS (language), and GraphCL and MVGRL on graph datasets.},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Zhen and Huang, Tinglin and Ding, Ming and Dong, Yuxiao and Ying, Rex and Cen, Yukuo and Geng, Yangliao and Tang, Jie},
	month = aug,
	year = {2023},
	pages = {3057--3069},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\5NIHPJYR\\Yang et al. - 2023 - BatchSampler Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs.pdf:application/pdf},
}


@article{ji_survey_2023,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2025-06-08},
	journal = {ACM Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	pages = {248:1--248:38},
	file = {Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\KXC5FFAE\\Ji et al. - 2023 - Survey of Hallucination in Natural Language Generation.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2025-06-08},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\KEV6GLK6\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\5822Q5TU\\2302.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-06-08},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\BEJ8VBZZ\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\UCX9RURQ\\2005.html:text/html},
}

@inproceedings{alexnet,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'12},
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {1},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2025-06-08},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = dec,
	year = {2012},
	pages = {1097--1105},
}

@misc{triplet,
	title = {Deep metric learning using {Triplet} network},
	url = {http://arxiv.org/abs/1412.6622},
	doi = {10.48550/arXiv.1412.6622},
	abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Hoffer, Elad and Ailon, Nir},
	month = dec,
	year = {2018},
	note = {arXiv:1412.6622 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\KPPLUPQE\\1412.html:text/html},
}

@misc{quadruplet,
	title = {Beyond triplet loss: a deep quadruplet network for person re-identification},
	shorttitle = {Beyond triplet loss},
	url = {http://arxiv.org/abs/1704.01719},
	doi = {10.48550/arXiv.1704.01719},
	abstract = {Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to CVPR2017},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\YWM3EGHE\\Chen et al. - 2017 - Beyond triplet loss a deep quadruplet network for person re-identification.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\H73V372B\\1704.html:text/html},
}
