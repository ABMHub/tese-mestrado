
@article{liu_document_2021,
	title = {Document image classification: {Progress} over two decades},
	volume = {453},
	issn = {0925-2312},
	shorttitle = {Document image classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221006925},
	doi = {10.1016/j.neucom.2021.04.114},
	abstract = {Document image classification plays a vital role in the document image processing system. Thus it is of great importance to have a clear understanding of the state-of-the-art of the document image classification field, especially in this deep learning era, which will facilitate the development of effective document image processing systems. In this paper, we provide a comprehensive survey of the progress that has been made in the field of document image classification over the past two decades. We categorize the document images into non-mobile images and mobile images according to the way they are acquired. The existing document image classification methods for these two types of images are reviewed, which are classified as textual-based methods, structural-based methods, visual-based methods and hybrid methods. We further compare the performance of different classification methods on several public benchmark datasets. Finally, we highlight some open issues and recommend promising directions for future research.},
	urldate = {2025-03-06},
	journal = {Neurocomputing},
	author = {Liu, Li and Wang, Zhiyu and Qiu, Taorong and Chen, Qiu and Lu, Yue and Suen, Ching Y.},
	month = sep,
	year = {2021},
	keywords = {Document image classification, Mobile document images, Non-mobile document images, Survey},
	pages = {223--240},
	file = {ScienceDirect Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\3XZXVBBD\\S0925231221006925.html:text/html},
	language = {english},
}

@article{abdallah_survey_2024,
	title = {A survey of recent approaches to form understanding in scanned documents},
	volume = {57},
	doi = {10.1007/s10462-024-11000-0},
	journal = {Artificial Intelligence Review},
	author = {Abdallah, A. and Eberharter, D. and Pfister, Z. and Jatowt, A.},
	year = {2024},
	pages = {342},
	language = {english},
}

@inproceedings{yu_documentnet_2023,
	title = {{DocumentNet}: {Bridging} the {Data} {Gap} in {Document} {Pre}-{Training}},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Yu, L. and Miao, J. and Sun, X. and {others}},
	year = {2023},
	pages = {707--722},
	language = {english},
}

@inproceedings{harley_evaluation_2015,
	title = {Evaluation of deep convolutional nets for document image classification},
	booktitle = {2015 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Harley, A. W. and Ufkes, A. and Derpanis, K. G.},
	year = {2015},
	language = {english},
	pages = {991--995},
}

@article{sarkhel_cross-modal_2024,
	title = {Cross-{Modal} {Entity} {Matching} for {Visually} {Rich} {Documents}},
	volume = {arXiv:2303.00720},
	journal = {arXiv preprint},
	author = {Sarkhel, R. and Nandi, A.},
	year = {2024},
	language = {english},
}

@article{song_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Few}-shot {Learning}: {Evolution}, {Applications}, {Challenges}, and {Opportunities}},
	volume = {arXiv:2205.06743},
	journal = {arXiv preprint},
	author = {Song, Y. and Wang, T. and Mondal, S. and Sahoo, J. P.},
	year = {2022},
	file = {Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\EU3ISX2S\\Song et al. - 2023 - A Comprehensive Survey of Few-shot Learning Evolution, Applications, Challenges, and Opportunities.pdf:application/pdf},
	language = {english},
}

@inproceedings{mathew_docvqa_2021,
	title = {{DocVQA}: {A} dataset for {VQA} on document images},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Mathew, M. and Kembhavi, A. and Schreiber, J. and Batra, D. and Parikh, D. and Bansal, M.},
	year = {2021},
	pages = {2200--2209},
	language = {english},
}

@inproceedings{jaume_funsd_2019,
	title = {{FUNSD}: {A} dataset for form understanding in noisy scanned documents},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Jaume, G. and Ekenel, H. K. and Thiran, J. P.},
	year = {2019},
	keywords = {Semantics, Task analysis, Optical character recognition software, Text recognition, Form Understanding, Google, Layout, Noise measurement, Optical Character Recognition, Spatial Layout Analysis, Text detection},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lucas\\Zotero\\storage\\ILNRVK37\\8892998.html:text/html;Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\99DPSPPZ\\Jaume et al. - 2019 - FUNSD A Dataset for Form Understanding in Noisy Scanned Documents.pdf:application/pdf},
	language = {english},
}

@inproceedings{huang_sroie_2019,
	title = {{SROIE}: {A} benchmark dataset for scanned receipts {OCR} and information extraction},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Huang, Z. and Jin, L. and Liu, X.},
	year = {2019},
	pages = {993--997},
	language = {english},
}

@techreport{lewis_iit-cdip_2006,
	title = {The {IIT}-{CDIP} test collection},
	institution = {Illinois Institute of Technology},
	author = {Lewis, D. D. and Agarwal, S. and Kappagantula, N. and Vora, P.},
	year = {2006},
	language = {english},
}

@inproceedings{zhou_document_2022,
	title = {Document {Layout} {Analysis} {Via} {Positional} {Encoding}},
	booktitle = {2022 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Zhou, E. and Wu, X. and Xiao, L. and Du, X. and Ma, T. and He, L.},
	year = {2022},
	pages = {1157--1160},
	language = {english},
}

@inproceedings{kim_text_2024,
	title = {Text {Role} {Classification} in {Scientific} {Charts} {Using} {Multimodal} {Transformers}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kim, H. J. and Lell, N. and Scherp, A.},
	year = {2024},
	language = {english},
}

@inproceedings{youssef_document_2022,
	title = {Document layout analysis with variational autoencoders: {An} industrial application},
	booktitle = {International {Symposium} on {Methodologies} for {Intelligent} {Systems}},
	publisher = {Springer},
	author = {Youssef, Ali and Valvano, Gabriele and Veneri, Giacomo},
	year = {2022},
	pages = {477--486},
	language = {english},
}

@inproceedings{kothandaraman_salad_2023,
	title = {Salad: {Source}-free active label-agnostic domain adaptation for classification, segmentation and detection},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Kothandaraman, Divya and Shekhar, Sumit and Sancheti, Abhilasha and Ghuhan, Manoj and Shukla, Tripti and Manocha, Dinesh},
	year = {2023},
	pages = {382--391},
	language = {english},
}

@inproceedings{sinha_cica_2024,
	title = {{CICA}: {Content}-{Injected} {Contrastive} {Alignment} for {Zero}-{Shot} {Document} {Image} {Classification}},
	booktitle = {International {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {Springer},
	author = {Sinha, Sankalp and Khan, Muhammad Saif Ullah and Sheikh, Talha Uddin and Stricker, Didier and Afzal, Muhammad Zeshan},
	year = {2024},
	pages = {124--141},
	language = {english},
}

@inproceedings{van_landeghem_distildoc_2024,
	title = {{DistilDoc}: {Knowledge} distillation for visually-rich document applications},
	booktitle = {International {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {Springer},
	author = {Van Landeghem, Jordy and Maity, Subhajit and Banerjee, Ayan and Blaschko, Matthew and Moens, Marie-Francine and Lladós, Josep and Biswas, Sanket},
	year = {2024},
	pages = {195--217},
	language = {english},
}

@inproceedings{zeghidi_cdp-sim_2023,
	title = {{CDP}-{Sim}: {Similarity} metric learning to identify the fake {Copy} {Detection} {Patterns}},
	booktitle = {2023 {IEEE} {International} {Workshop} on {Information} {Forensics} and {Security} ({WIFS})},
	publisher = {IEEE},
	author = {Zeghidi, Hédi and Crispim-Junior, Carlos and Tkachenko, Iuliia},
	year = {2023},
	pages = {1--6},
	language = {english},
}

@article{li_few-shot_2021,
	title = {Few-shot prototype alignment regularization network for document image layout segementation},
	volume = {115},
	journal = {Pattern Recognition},
	author = {Li, Yujie and Zhang, Pengfei and Xu, Xing and Lai, Yi and Shen, Fumin and Chen, Lijiang and Gao, Pengxiang},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {107882},
	language = {english},
}

@article{jingnan_intention-guided_2023,
	title = {Intention-guided deep semi-supervised document clustering via metric learning},
	volume = {35},
	number = {1},
	journal = {Journal of King Saud University-Computer and Information Sciences},
	author = {Jingnan, Li and Chuan, Lin and Ruizhang, Huang and Yongbin, Qin and Yanping, Chen},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {416--425},
	language = {english},
}

@inproceedings{zhu_layout_2023,
	title = {Layout similarity based key information extraction framework for structural images},
	booktitle = {2023 {International} {Conference} on {Image} {Processing}, {Computer} {Vision} and {Machine} {Learning} ({ICICML})},
	publisher = {IEEE},
	author = {Zhu, Maosheng and Ni, Ruijie},
	year = {2023},
	pages = {181--184},
	language = {english},
}

@article{ward_jr_hierarchical_1963,
	title = {Hierarchical grouping to optimize an objective function},
	volume = {58},
	number = {301},
	journal = {Journal of the American statistical association},
	author = {Ward Jr, Joe H},
	year = {1963},
	note = {Publisher: Taylor \& Francis},
	pages = {236--244},
	language = {english},
}

@inproceedings{xian_feature_2018,
	title = {Feature generating networks for zero-shot learning},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Xian, Yongqin and Lorenz, Tobias and Schiele, Bernt and Akata, Zeynep},
	year = {2018},
	pages = {5542--5551},
	language = {english},
}

@article{chen_expanding_2024,
	title = {Expanding {Performance} {Boundaries} of {Open}-{Source} {Multimodal} {Models} with {Model}, {Data}, and {Test}-{Time} {Scaling}},
	journal = {arXiv e-prints},
	author = {Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and {others}},
	year = {2024},
	pages = {arXiv--2412},
	language = {english},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {https://arxiv.org/abs/2303.08774},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and {Michael} and {Pokorny} and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	year = {2024},
	note = {\_eprint: 2303.08774},
	language = {english},
}

@article{robbins_stochastic_1951,
	title = {A stochastic approximation method},
	journal = {The annals of mathematical statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	year = {1951},
	note = {Publisher: JSTOR},
	pages = {400--407},
	language = {english},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {https://arxiv.org/abs/1412.6980},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2017},
	note = {\_eprint: 1412.6980},
	language = {english},
}

@inproceedings{liu_super_2022,
	title = {Super convergence cosine annealing with warm-up learning rate},
	booktitle = {{CAIBDA} 2022; 2nd {International} {Conference} on {Artificial} {Intelligence}, {Big} {Data} and {Algorithms}},
	publisher = {VDE},
	author = {Liu, Zhao},
	year = {2022},
	pages = {1--7},
	language = {english},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	doi = {10.1109/CVPR.2009.5206848},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
	language = {english},
}

@article{wong_reliable_2019,
	title = {Reliable accuracy estimates from k-fold cross validation},
	volume = {32},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wong, Tzu-Tsung and Yeh, Po-Yang},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {1586--1594},
	language = {english},
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {https://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
	urldate = {2025-03-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6105--6114},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\J3FYUBC9\\Tan e Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:application/pdf},
	language = {english},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/7780459},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-03-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lucas\\Zotero\\storage\\WFTBH7G3\\7780459.html:text/html;Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\S9KGQ6HE\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
	language = {english},
}

@article{kay_tesseract_2007,
	title = {Tesseract: an open-source optical character recognition engine},
	volume = {2007},
	issn = {1075-3583},
	shorttitle = {Tesseract},
	abstract = {If you really need OCR.},
	number = {159},
	journal = {Linux J.},
	author = {Kay, Anthony},
	month = jul,
	year = {2007},
	pages = {2},
	language = {english},
}

@article{xian_zero-shot_2019,
	title = {Zero-{Shot} {Learning}—{A} {Comprehensive} {Evaluation} of the {Good}, the {Bad} and the {Ugly}},
	volume = {41},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/8413121},
	doi = {10.1109/TPAMI.2018.2857768},
	abstract = {Due to the importance of zero-shot learning, i.e., classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g., pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.},
	number = {9},
	urldate = {2025-03-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xian, Yongqin and Lampert, Christoph H. and Schiele, Bernt and Akata, Zeynep},
	month = sep,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Fish, Generalized zero-shot learning, image classification, Learning systems, Protocols, Semantics, Task analysis, Training, transductive learning, Visualization, weakly-supervised learning},
	pages = {2251--2265},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lucas\\Zotero\\storage\\VFHPGE9R\\8413121.html:text/html;Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\KKRZSCR7\\Xian et al. - 2019 - Zero-Shot Learning—A Comprehensive Evaluation of the Good, the Bad and the Ugly.pdf:application/pdf},
	language = {english},
}

@misc{howard_searching_2019,
	title = {Searching for {MobileNetV3}},
	url = {http://arxiv.org/abs/1905.02244},
	doi = {10.48550/arXiv.1905.02244},
	abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2{\textbackslash}\% more accurate on ImageNet classification while reducing latency by 15{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Small is 4.6{\textbackslash}\% more accurate while reducing latency by 5{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Large detection is 25{\textbackslash}\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30{\textbackslash}\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	month = nov,
	year = {2019},
	note = {arXiv:1905.02244 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2019},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\DBHAYNHZ\\Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\WFTEZBPT\\1905.html:text/html},
	language = {english},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	doi = {10.48550/arXiv.2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\6PRB56W8\\Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\DKECN79P\\2004.html:text/html},
	language = {english},
}

@inproceedings{chopra_learning_2005,
	title = {Learning a similarity metric discriminatively, with application to face verification},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/1467314},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
	urldate = {2025-03-06},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Artificial neural networks, Character generation, Drives, Face recognition, Glass, Robustness, Spatial databases, Support vector machine classification, Support vector machines, System testing},
	pages = {539--546 vol. 1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lucas\\Zotero\\storage\\B5SV9QDD\\1467314.html:text/html},
	language = {english},
}

@inproceedings{jha_transformer-based_2023,
	title = {Transformer-based {Models} for {Long}-{Form} {Document} {Matching}: {Challenges} and {Empirical} {Analysis}},
	shorttitle = {Transformer-based {Models} for {Long}-{Form} {Document} {Matching}},
	url = {https://doi.org/10.18653/v1/2023.findings-eacl.178},
	doi = {10.18653/V1/2023.FINDINGS-EACL.178},
	urldate = {2025-03-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023, {Dubrovnik}, {Croatia}, {May} 2-6, 2023},
	publisher = {Association for Computational Linguistics},
	author = {Jha, Akshita and Samavedhi, Adithya and Rakesh, Vineeth and Chandrashekar, Jaideep and Reddy, Chandan K.},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	year = {2023},
	pages = {2300--2310},
	file = {Texto completo:C\:\\Users\\lucas\\Zotero\\storage\\FL823HPC\\Jha et al. - 2023 - Transformer-based Models for Long-Form Document Matching Challenges and Empirical Analysis.pdf:application/pdf},
	language = {english},
}

@inproceedings{huang_icdar2019_2019,
	title = {{ICDAR2019} {Competition} on {Scanned} {Receipt} {OCR} and {Information} {Extraction}},
	url = {https://ieeexplore.ieee.org/document/8977955},
	doi = {10.1109/ICDAR.2019.00244},
	abstract = {The ICDAR 2019 Challenge on "Scanned receipts OCR and key information extraction" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field.},
	urldate = {2025-03-07},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, C. V.},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Deep Learning, Information Extraction, Information retrieval, OCR, Optical character recognition software, Protocols, Research and development, Scanned Receipt, Task analysis, Text analysis, Text recognition},
	pages = {1516--1520},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lucas\\Zotero\\storage\\UYHSJWZL\\8977955.html:text/html;Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\CKVZTKV5\\Huang et al. - 2019 - ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction.pdf:application/pdf},
	language = {english},
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	urldate = {2025-03-07},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
	language = {english},

}

@incollection{chicco_siamese_2021,
	address = {New York, NY},
	title = {Siamese {Neural} {Networks}: {An} {Overview}},
	isbn = {978-1-0716-0826-5},
	shorttitle = {Siamese {Neural} {Networks}},
	url = {https://doi.org/10.1007/978-1-0716-0826-5_3},
	abstract = {Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman’s rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.},
	urldate = {2025-03-08},
	booktitle = {Artificial {Neural} {Networks}},
	publisher = {Springer US},
	author = {Chicco, Davide},
	editor = {Cartwright, Hugh},
	year = {2021},
	doi = {10.1007/978-1-0716-0826-5_3},
	pages = {73--94},
	language = {english},
}

@inproceedings{scius-bertrand_zero-shot_2025,
	address = {Cham},
	title = {Zero-{Shot} {Prompting} and {Few}-{Shot} {Fine}-{Tuning}: {Revisiting} {Document} {Image} {Classification} {Using} {Large} {Language} {Models}},
	isbn = {978-3-031-78495-8},
	shorttitle = {Zero-{Shot} {Prompting} and {Few}-{Shot} {Fine}-{Tuning}},
	doi = {10.1007/978-3-031-78495-8_10},
	abstract = {Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect performance when considering hundreds of thousands of training samples. With the advent of large language models (LLMs), which are excellent few-shot learners, the question arises to what extent the document classification problem can be addressed with only a few training samples, or even none at all. In this paper, we investigate this question in the context of zero-shot prompting and few-shot model fine-tuning, with the aim of reducing the need for human-annotated training samples as much as possible.},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer Nature Switzerland},
	author = {Scius-Bertrand, Anna and Jungo, Michael and Vögtlin, Lars and Spat, Jean-Marc and Fischer, Andreas},
	editor = {Antonacopoulos, Apostolos and Chaudhuri, Subhasis and Chellappa, Rama and Liu, Cheng-Lin and Bhattacharya, Saumik and Pal, Umapada},
	year = {2025},
	pages = {152--166},
	language = {english},
}

@article{bakkali_eaml_2021,
	title = {{EAML}: ensemble self-attention-based mutual learning network for document image classification},
	volume = {24},
	issn = {1433-2833},
	shorttitle = {{EAML}},
	url = {https://doi.org/10.1007/s10032-021-00378-0},
	doi = {10.1007/s10032-021-00378-0},
	abstract = {In the recent past, complex deep neural networks have received huge interest in various document understanding tasks such as document image classification and document retrieval. As many document types have a distinct visual style, learning only visual features with deep CNNs to classify document images has encountered the problem of low inter-class discrimination, and high intra-class structural variations between its categories. In parallel, text-level understanding jointly learned with the corresponding visual properties within a given document image has considerably improved the classification performance in terms of accuracy. In this paper, we design a self-attention-based fusion module that serves as a block in our ensemble trainable network. It allows to simultaneously learn the discriminant features of image and text modalities throughout the training stage. Besides, we encourage mutual learning by transferring the positive knowledge between image and text modalities during the training stage. This constraint is realized by adding a truncated Kullback–Leibler divergence loss (Tr-KLDReg) as a new regularization term, to the conventional supervised setting. To the best of our knowledge, this is the first time to leverage a mutual learning approach along with a self-attention-based fusion module to perform document image classification. The experimental results illustrate the effectiveness of our approach in terms of accuracy for the single-modal and multi-modal modalities. Thus, the proposed ensemble self-attention-based mutual learning model outperforms the state-of-the-art classification results based on the benchmark RVL-CDIP and Tobacco-3482 datasets.},
	number = {3},
	urldate = {2025-06-03},
	journal = {Int. J. Doc. Anal. Recognit.},
	author = {Bakkali, Souhail and Ming, Zuheng and Coustaty, Mickaël and Rusiñol, Marçal},
	month = sep,
	year = {2021},
	keywords = {Artificial Intelligence, Automated Pattern Recognition, Categorization, Ensemble learning, Machine Learning, Multi-modal fusion, Mutual learning, Object Recognition, Self-attention-based fusion, Statistical Learning, Text document image classification},
	pages = {251--268},
	language = {english},
}

@inproceedings{yang_batchsampler_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {{BatchSampler}: {Sampling} {Mini}-{Batches} for {Contrastive} {Learning} in {Vision}, {Language}, and {Graphs}},
	isbn = {979-8-4007-0103-0},
	shorttitle = {{BatchSampler}},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599263},
	doi = {10.1145/3580305.3599263},
	abstract = {In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives within the current mini-batch, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler{\textbackslash}footnoteThe code is available at BatchSampler to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sample hard-to-distinguish instances. BatchSampler is a simple and general technique that can be directly plugged into existing contrastive learning models in vision, language, and graphs. Extensive experiments on datasets of three modalities show that BatchSampler can consistently improve the performance of powerful contrastive models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE on STS (language), and GraphCL and MVGRL on graph datasets.},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Zhen and Huang, Tinglin and Ding, Ming and Dong, Yuxiao and Ying, Rex and Cen, Yukuo and Geng, Yangliao and Tang, Jie},
	month = aug,
	year = {2023},
	pages = {3057--3069},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\5NIHPJYR\\Yang et al. - 2023 - BatchSampler Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs.pdf:application/pdf},
	language = {english},
}

@article{ji_survey_2023,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2025-06-08},
	journal = {ACM Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	pages = {248:1--248:38},
	file = {Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\KXC5FFAE\\Ji et al. - 2023 - Survey of Hallucination in Natural Language Generation.pdf:application/pdf},
	language = {english},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2025-06-08},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\KEV6GLK6\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\5822Q5TU\\2302.html:text/html},
	language = {english},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-06-08},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\BEJ8VBZZ\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\UCX9RURQ\\2005.html:text/html},
	language = {english},

}

@inproceedings{krizhevsky_imagenet_2012,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'12},
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {1},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2025-06-08},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = dec,
	year = {2012},
	pages = {1097--1105},
	language = {english},
}

@misc{hoffer_deep_2018,
	title = {Deep metric learning using {Triplet} network},
	url = {http://arxiv.org/abs/1412.6622},
	doi = {10.48550/arXiv.1412.6622},
	abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Hoffer, Elad and Ailon, Nir},
	month = dec,
	year = {2018},
	note = {arXiv:1412.6622 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\HQFTIS83\\Hoffer e Ailon - 2018 - Deep metric learning using Triplet network.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\KPPLUPQE\\1412.html:text/html},
	language = {english},

}

@misc{chen_beyond_2017,
	title = {Beyond triplet loss: a deep quadruplet network for person re-identification},
	shorttitle = {Beyond triplet loss},
	url = {http://arxiv.org/abs/1704.01719},
	doi = {10.48550/arXiv.1704.01719},
	abstract = {Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to CVPR2017},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\YWM3EGHE\\Chen et al. - 2017 - Beyond triplet loss a deep quadruplet network for person re-identification.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\H73V372B\\1704.html:text/html},
	language = {english},

}

@article{jiang_dsdtw_2022,
	title = {{DsDTW}: {Local} {Representation} {Learning} {With} {Deep} soft-{DTW} for {Dynamic} {Signature} {Verification}},
	volume = {17},
	issn = {1556-6021},
	shorttitle = {{DsDTW}},
	url = {https://ieeexplore.ieee.org/document/9787558},
	doi = {10.1109/TIFS.2022.3180219},
	abstract = {Dynamic time warping (DTW) is a popular technique for sequence alignment, and is the de facto standard for dynamic signature verification. In this paper, we go a significant step further to enhance DTW with the capability of deep representation learning, and propose an end-to-end trainable Deep soft-DTW (DsDTW) model for dynamic signature verification. Specifically, we design a convolutional recurrent adaptive network (CRAN) to process dynamic signatures, and utilize it to provide robust and discriminative local representations as inputs for DTW. As DTW is not fully differentiable with regard to its inputs, we introduce its smoothed formulation, soft-DTW, and incorporate the soft-DTW distances of signature pairs into the loss function for optimization. Because soft-DTW is differentiable, the proposed DsDTW is end-to-end trainable, and achieves an elegant integration of CRAN deep learning model and traditional DTW mechanism. Our method achieves state-of-the-art performance on several public benchmarks, and has won first place in the ICDAR 2021 competition for online signature verification. Source codes of DsDTW is available at https://github.com/KAKAFEI123/DsDTW.},
	urldate = {2025-06-14},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Jiang, Jiajia and Lai, Songxuan and Jin, Lianwen and Zhu, Yecheng},
	year = {2022},
	keywords = {Training, Task analysis, Adaptive systems, Benchmark testing, convolutional recurrent adaptive network, Deep learning, deep representation learning, Dynamic signature verification, dynamic time warping, Feature extraction, Handwriting recognition, soft-DTW},
	pages = {2198--2212},
	language = {english},

}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-SNE" that visualizes
high-dimensional data by giving each datapoint a location in a two or
three-dimensional map. The technique is a variation of Stochastic
Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize,
and produces significantly better visualizations by reducing the
tendency to crowd points together in the center of the map. t-SNE is
better than existing techniques at creating a single map that reveals
structure at many different scales. This is particularly important for
high-dimensional data that lie on several different, but related,
low-dimensional manifolds, such as images ofobjects from multiple
classes seen from multiple viewpoints. For visualizing the structure
of very large data sets, we show how t-SNE can use random walks on
neighborhood graphs to allow the implicit structure of all of the data
to influence the way in which a subset of the data is displayed. We
illustrate the performance of t-SNE on a wide variety of data sets and
compare it with many other non-parametric visualization techniques,
including Sammon mapping, Isomap, and Locally Linear Embedding. The
visualizations produced by t-SNE are significantly better than those
produced by the other techniques on almost all of the data sets.},
	number = {86},
	urldate = {2025-06-15},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\NWNTNBL2\\Maaten e Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf},
	language = {english},

}

@inproceedings{bakkali_visual_2020,
	title = {Visual and {Textual} {Deep} {Feature} {Fusion} for {Document} {Image} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9150829},
	doi = {10.1109/CVPRW50498.2020.00289},
	abstract = {The topic of text document image classification has been explored extensively over the past few years. Most recent approaches handled this task by jointly learning the visual features of document images and their corresponding textual contents. Due to the various structures of document images, the extraction of semantic information from its textual content is beneficial for document image processing tasks such as document retrieval, information extraction, and text classification. In this work, a two-stream neural architecture is proposed to perform the document image classification task. We conduct an exhaustive investigation of nowadays widely used neural networks as well as word embedding procedures used as backbones, in order to extract both visual and textual features from document images. Moreover, a joint feature learning approach that combines image features and text embeddings is introduced as a late fusion methodology. Both the theoretical analysis and the experimental results demonstrate the superiority of our proposed joint feature learning method comparatively to the single modalities. This joint learning approach outperforms the state-of-the-art results with a classification accuracy of 97.05\% on the large-scale RVL-CDIP dataset.},
	urldate = {2025-06-16},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Bakkali, Souhail and Ming, Zuheng and Coustaty, Mickaël and Rusiñol, Marçal},
	month = jun,
	year = {2020},
	note = {ISSN: 2160-7516},
	keywords = {Neural networks, Visualization, Semantics, Task analysis, Optical character recognition software, Feature extraction, Streaming media},
	pages = {2394--2403},
	annote = {ISSN: 2160-7516},
	file = {Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\3R5SD4ZD\\9150829.html:text/html;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\Y236V3QG\\9150829.html:text/html},
	language = {english},
}

@inproceedings{larson_evaluation_2023,
	address = {Dubrovnik, Croatia},
	title = {On {Evaluation} of {Document} {Classification} with {RVL}-{CDIP}},
	url = {https://aclanthology.org/2023.eacl-main.195/},
	doi = {10.18653/v1/2023.eacl-main.195},
	abstract = {The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1\% (ranging between 1.6\% to 16.9\% per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recommendations for what characteristics such a resource should include.},
	urldate = {2025-06-17},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Larson, Stefan and Lim, Gordon and Leach, Kevin},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {2665--2678},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\LHNUN2KB\\Larson et al. - 2023 - On Evaluation of Document Classification with RVL-CDIP.pdf:application/pdf},
	language = {english},
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	shorttitle = {{FaceNet}},
	url = {https://ieeexplore.ieee.org/document/7298682},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	urldate = {2025-06-17},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Training, Artificial neural networks, Face recognition, Accuracy, Face, Principal component analysis, Standards},
	pages = {815--823},
	annote = {ISSN: 1063-6919},
	file = {Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\Y2493IF5\\7298682.html:text/html;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\ZT26TZP2\\7298682.html:text/html;Versão submetida:C\:\\Users\\lucas\\Zotero\\storage\\DZWSYWLM\\Schroff et al. - 2015 - FaceNet A unified embedding for face recognition and clustering.pdf:application/pdf},
	language = {english},
}

@inproceedings{pfitzmann_doclaynet_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {{DocLayNet}: {A} {Large} {Human}-{Annotated} {Dataset} for {Document}-{Layout} {Segmentation}},
	isbn = {978-1-4503-9385-0},
	shorttitle = {{DocLayNet}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539043},
	doi = {10.1145/3534678.3539043},
	abstract = {Accurate document layout analysis is a key requirement for high-quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we presentDocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10\% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet-trained models are more robust and thus the preferred choice for general-purpose document-layout analysis.},
	urldate = {2025-06-20},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S. and Staar, Peter},
	month = aug,
	year = {2022},
	pages = {3743--3751},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\IRYUCYEK\\Pfitzmann et al. - 2022 - DocLayNet A Large Human-Annotated Dataset for Document-Layout Segmentation.pdf:application/pdf},
	language = {english},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2025-06-21},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\TA34CLGJ\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\27M7WJJ6\\1912.html:text/html},
	language = {english},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-06-21},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {Preprint PDF:C\:\\Users\\lucas\\Zotero\\storage\\ELCRCJB5\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\lucas\\Zotero\\storage\\AKXFU36I\\1706.html:text/html},
	language = {english},
}

@techreport{settles_active_2009,
	type = {Technical {Report}},
	title = {Active {Learning} {Literature} {Survey}},
	url = {https://minds.wisconsin.edu/handle/1793/60660},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain.

This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	urldate = {2025-10-15},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	author = {Settles, Burr},
	year = {2009},
	note = {Accepted: 2012-03-15T17:23:56Z},
	file = {Full Text PDF:C\:\\Users\\lucas\\Zotero\\storage\\D9AM6QHY\\Settles - 2009 - Active Learning Literature Survey.pdf:application/pdf},
	language = {english},
}

@inproceedings{zhong_publaynet_2019,
	title = {{PubLayNet}: {Largest} {Dataset} {Ever} for {Document} {Layout} {Analysis}},
	url = {https://doi.org/10.1109/ICDAR.2019.00166},
	doi = {10.1109/ICDAR.2019.00166},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2019, {Sydney}, {Australia}, {September} 20-25, 2019},
	publisher = {IEEE},
	author = {Zhong, Xu and Tang, Jianbin and Jimeno-Yepes, Antonio},
	year = {2019},
	pages = {1015--1022},
	language = {english},

}

@inproceedings{harley_evaluation_2015-1,
	title = {Evaluation of deep convolutional nets for document image classification and retrieval},
	url = {https://doi.org/10.1109/ICDAR.2015.7333910},
	doi = {10.1109/ICDAR.2015.7333910},
	booktitle = {13th {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2015, {Nancy}, {France}, {August} 23-26, 2015},
	publisher = {IEEE Computer Society},
	author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
	year = {2015},
	pages = {991--995},
	language = {english},
}

@inproceedings{lewis_building_2006,
	title = {Building a test collection for complex document information processing},
	url = {https://doi.org/10.1145/1148170.1148307},
	doi = {10.1145/1148170.1148307},
	booktitle = {{SIGIR} 2006: {Proceedings} of the 29th {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}, {Seattle}, {Washington}, {USA}, {August} 6-11, 2006},
	publisher = {ACM},
	author = {Lewis, David D. and Agam, Gady and Argamon, Shlomo and Frieder, Ophir and Grossman, David A. and Heard, Jefferson},
	editor = {Efthimiadis, Efthimis N. and Dumais, Susan T. and Hawking, David and Järvelin, Kalervo},
	year = {2006},
	pages = {665--666},
	language = {english},
}

@inproceedings{agrawal_hybrid_2014,
	title = {A hybrid partial fingerprint matching algorithm for estimation of {Equal} error rate},
	doi = {10.1109/ICACCCT.2014.7019308},
	booktitle = {2014 {IEEE} {International} {Conference} on {Advanced} {Communications}, {Control} and {Computing} {Technologies}},
	author = {Agrawal, Pinki and Kapoor, Ravikant and Agrawal, Sanjay},
	year = {2014},
	keywords = {FAR, Fingerprint recognition, FRR, Guidelines, Image segmentation, Local Binary Pattern, Minutiae extraction, Partial fingerprint, Physiology, Pores features, Radon transform, Score level fusion, Silicon, Transforms},
	pages = {1295--1299},
	language = {english},

}

@inproceedings{hofbauer_calculating_2016,
	title = {Calculating a boundary for the significance from the equal-error rate},
	doi = {10.1109/ICB.2016.7550053},
	booktitle = {2016 {International} {Conference} on {Biometrics} ({ICB})},
	author = {Hofbauer, Heinz and Uhl, Andreas},
	year = {2016},
	keywords = {Computers, Data mining, Databases, Error analysis, Estimation, Iris recognition, Partitioning algorithms},
	pages = {1--4},
	language = {english},
}

@inproceedings{tolosana_icdar_2021,
	title = {{ICDAR} 2021 competition on on-line signature verification},
	booktitle = {Document {Analysis} and {Recognition}–{ICDAR} 2021: 16th {International} {Conference}, {Lausanne}, {Switzerland}, {September} 5–10, 2021, {Proceedings}, {Part} {IV} 16},
	publisher = {Springer},
	author = {Tolosana, Ruben and Vera-Rodriguez, Ruben and Gonzalez-Garcia, Carlos and Fierrez, Julian and Rengifo, Santiago and Morales, Aythami and Ortega-Garcia, Javier and Carlos Ruiz-Garcia, Juan and Romero-Tapiador, Sergio and Jiang, Jiajia and {others}},
	year = {2021},
	pages = {723--737},
	language = {english},
}

@inproceedings{sevim_document_2022,
	address = {Cham},
	title = {Document {Image} {Classification} with {Vision} {Transformers}},
	isbn = {978-3-031-01984-5},
	booktitle = {Electrical and {Computer} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Sevim, Semih and Omurca, Sevinç {\textbackslash}.Ilhan and Ekinci, Ekin},
	editor = {Seyman, Muhammet Nuri},
	year = {2022},
	pages = {68--81},
	language = {english},
}

@article{li_intention-guided_2023,
	title = {Intention-guided deep semi-supervised document clustering via metric learning},
	volume = {35},
	url = {https://doi.org/10.1016/j.jksuci.2022.12.010},
	doi = {10.1016/J.JKSUCI.2022.12.010},
	number = {1},
	journal = {J. King Saud Univ. Comput. Inf. Sci.},
	author = {Li, Jingnan and Lin, Chuan and Huang, Ruizhang and Qin, Yongbin and Chen, Yanping},
	year = {2023},
	pages = {416--425},
	language = {english},
}

@article{jr_hierarchical_1963,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}},
	volume = {58},
	url = {https://doi.org/10.1080/01621459.1963.10500845},
	doi = {10.1080/01621459.1963.10500845},
	number = {301},
	journal = {Journal of the American Statistical Association},
	author = {Jr, Joe H. Ward},
	year = {1963},
	pages = {236--244},
	language = {english},
}

@misc{kingma_adam_2015,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
	note = {Publication Title: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	language = {english},
}

@inproceedings{park_cord_2019,
	title = {{CORD}: {A} {Consolidated} {Receipt} {Dataset} for {Post}-{OCR} {Parsing}},
	url = {https://arxiv.org/abs/1908.07414},
	booktitle = {Document {Intelligence} {Workshop} at {NeurIPS} 2019},
	author = {Park, Seunghyun and Shin, Seung and Kim, Byeongchang and Cha, Junbum and Lee, Hwalsuk},
	year = {2019},
	language = {english},
}

@article{xu_layoutxlm_2021,
	title = {{LayoutXLM}: {Multimodal} {Pre}-training for {Multilingual} {Visually}-rich {Document} {Understanding}},
	volume = {abs/2104.08836},
	url = {https://arxiv.org/abs/2104.08836},
	journal = {CoRR},
	author = {Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wang, Guoxin and Lu, Yijuan and Florêncio, Dinei and Zhang, Cha and Wei, Furu},
	year = {2021},
	note = {arXiv: 2104.08836},
	language = {english},
}

@misc{ai_llama_2024,
	title = {Llama 3.2: {From} {Cloud} to {Edge}, {Now} with {Vision}},
	url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
	author = {AI, Meta},
	year = {2024},
	language = {english},

}

@misc{openai_hello_2024,
	title = {Hello {GPT}-4o},
	url = {https://openai.com/index/hello-gpt-4o/},
	author = {{OpenAI}},
	year = {2024},
	language = {english},
}

@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o mini: advancing cost-efficient intelligence},
	url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
	author = {{OpenAI}},
	year = {2024},
	language = {english},

}

@misc{mindee_doctr_2021,
	title = {{docTR}: {Document} {Text} {Recognition}},
	url = {https://github.com/mindee/doctr},
	publisher = {GitHub},
	author = {{Mindee}},
	year = {2021},
	language = {english},
}

@inproceedings{howard_searching_2019-1,
	title = {Searching for {MobileNetV3}},
	url = {https://doi.org/10.1109/ICCV.2019.00140},
	doi = {10.1109/ICCV.2019.00140},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}, {ICCV} 2019, {Seoul}, {Korea} ({South}), {October} 27 - {November} 2, 2019},
	publisher = {IEEE},
	author = {Howard, Andrew and Pang, Ruoming and Adam, Hartwig and Le, Quoc V. and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun},
	year = {2019},
	pages = {1314--1324},
	language = {english},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	url = {https://openreview.net/forum?id=YicbFdNTTy},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
	language = {english},
}

@article{wu_deepseek-vl2_2024,
	title = {{DeepSeek}-{VL2}: {Mixture}-of-{Experts} {Vision}-{Language} {Models} for {Advanced} {Multimodal} {Understanding}},
	volume = {abs/2412.10302},
	url = {https://doi.org/10.48550/arXiv.2412.10302},
	doi = {10.48550/ARXIV.2412.10302},
	journal = {CoRR},
	author = {Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and Xie, Zhenda and Wu, Yu and Hu, Kai and Wang, Jiawei and Sun, Yaofeng and Li, Yukun and Piao, Yishi and Guan, Kang and Liu, Aixin and Xie, Xin and You, Yuxiang and Dong, Kai and Yu, Xingkai and Zhang, Haowei and Zhao, Liang and Wang, Yisong and Ruan, Chong},
	year = {2024},
	note = {arXiv: 2412.10302},
	language = {english},
}

@article{bai_qwen25-vl_2025,
	title = {Qwen2.5-{VL} {Technical} {Report}},
	url = {https://arxiv.org/abs/2502.13923},
	journal = {arXiv preprint arXiv:2502.13923},
	author = {Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and {others}},
	year = {2025},
	language = {english},
}

@misc{research_google_2024,
	title = {Google {Colaboratory}},
	url = {https://colab.research.google.com/},
	author = {Research, Google},
	year = {2024},
	language = {english},
}

@article{li_survey_2025,
	title = {A {Survey} on {Deep} {Active} {Learning}: {Recent} {Advances} and {New} {Frontiers}},
	volume = {36},
	doi = {10.1109/TNNLS.2024.3396463},
	number = {4},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Li, Dongyuan and Wang, Zhen and Chen, Yankai and Jiang, Renhe and Ding, Weiping and Okumura, Manabu},
	year = {2025},
	keywords = {Training, Task analysis, Deep learning, Active learning, adaptive sampling, Annotations, computer vision (CV), deep learning, Electronic mail, natural language processing (NLP), sequential optimal design, Surveys, Taxonomy, uncertainty quantification},
	pages = {5879--5899},
	language = {english},
}

@inproceedings{fujinuma_multi-modal_2023,
	title = {A {Multi}-{Modal} {Multilingual} {Benchmark} for {Document} {Image} {Classification}},
	url = {https://doi.org/10.18653/v1/2023.findings-emnlp.958},
	doi = {10.18653/V1/2023.FINDINGS-EMNLP.958},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023, {Singapore}, {December} 6-10, 2023},
	publisher = {Association for Computational Linguistics},
	author = {Fujinuma, Yoshinari and Varia, Siddharth and Sankaran, Nishant and Appalaraju, Srikar and Min, Bonan and Vyas, Yogarshi},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	year = {2023},
	pages = {14361--14376},
	language = {english},
}

@inproceedings{radford_learning_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/radford21a.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {8748--8763},
	language = {english},
}

@inproceedings{khalifa_contrastive_2023,
	address = {Toronto, Canada},
	title = {Contrastive {Training} {Improves} {Zero}-{Shot} {Classification} of {Semi}-structured {Documents}},
	url = {https://aclanthology.org/2023.findings-acl.473/},
	doi = {10.18653/v1/2023.findings-acl.473},
	abstract = {We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new classification categories could potentially emerge. We focus exclusively on the zero-shot learning setting where inference is done on new unseen classes. To address this task, we propose a matching-based approach that relies on a pairwise contrastive objective for both pretraining and fine-tuning. Our results show a significant boost in Macro F1 from the proposed pretraining step and comparable performance of the contrastive fine-tuning to a standard prediction objective in both supervised and unsupervised zero-shot settings.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Khalifa, Muhammad and Vyas, Yogarshi and Wang, Shuai and Horwood, Graham and Mallya, Sunil and Ballesteros, Miguel},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {7499--7508},
	language = {english},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {https://arxiv.org/abs/2304.02643},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	year = {2023},
	note = {\_eprint: 2304.02643},
	language = {english},
}
