This chapter analyzes the literature about automatic document processing, focusing on the available datasets and document classification methods. The relevance and direction of this work are justified by the limitations of each dataset and classification framework. This chapter also helps to provide the boundaries for the problem.

\section{Document Understanding}

Document Understanding is a broad field encompassing various tasks that aim to extract and interpret information from documents, which may contain text, tables, images, and complex layouts~\cite{formUnderstandingSurvey}. The increasing digitization of documents has led to significant advancements in document analysis techniques, particularly through deep learning and transformer-based architectures. Among the key challenges in this domain is handling the inherent complexity of scanned documents, which often exhibit noise, structural variability, and diverse formatting styles.

Several fundamental tasks define the scope of document understanding. \textit{Document Layout Analysis} (DLA) involves detecting and categorizing different structural components of a document, such as text blocks, tables, images, and forms, to facilitate higher-level information extraction~\cite{zhong2019publaynet}. \gls{IE} focuses on retrieving relevant information, such as named entities and key-value pairs, from structured and unstructured document sources~\cite{formUnderstandingSurvey}. 

% Datasets and benchmarks have been developed to evaluate \glspl{DLA} and form understanding tasks, including FUNSD\cite{funsd2019}, CORD\cite{cord2019}, and SROIE\cite{huang_sroie_2019}, as highlighted in prior research~\cite{formUnderstandingSurvey}. These datasets focus on various aspects such as structured information extraction, OCR robustness, and structural parsing, providing essential baselines for model evaluation. However, while these benchmarks contribute to assessing specific tasks in document understanding, they do not fully address the challenges of layout-based similarity classification as explored in this work.

\section{Document Understanding Databases}

This subsection reviews existing datasets commonly used for document classification and discusses their limitations in supporting zero-shot scenarios. By analyzing these datasets, the need for a new benchmark that explicitly enforces zero-shot constraints is highlighted.

The IIT-CDIP dataset~\cite{iitcdip2006}, introduced in 2006, is a large-scale repository used for document classification and information retrieval. It originated from the Tobacco Documents Library, from the University of San Francisco Industry Documents Library collection, and contains millions of scanned documents. The most popular dataset when working with document-image classification is the RVL-CDIP dataset~\cite{harley2015rvlcdip}, which was introduced in 2015. This dataset is a labeled subset of the IIT-CDIP dataset, and organized 400,000 document images into 16 predefined categories such as letters, forms, and emails. This classification is driven by the document's purpose and uses, which hinders the performance of a ZSL model trained from scratch, with no real-world knowledge.

More recently, the DocVQA dataset~\cite{mathew2021docvqa} was introduced, leveraging a subset of documents from the same collection. It comprises over 12,000 document images paired with 50,000 question-answer pairs, designed to evaluate models’ abilities in visual question answering on document images. DocVQA has since become a standard benchmark for assessing the performance of \glspl{LLM} in document understanding tasks, particularly in multimodal reasoning scenarios. FUNSD dataset~\cite{funsd2019}, introduced in 2019, has been widely used for OCR-based semantic relation extraction from scanned forms. Similarly, SROIE~\cite{huang_icdar2019}, CORD~\cite{cord2019} and XFUND~\cite{xfund2021} target key-value pair extraction in receipts and invoices, focusing on structured entity retrieval, such as store names, monetary values, and transaction dates. While these tasks consider the organization of visual elements within documents, they do not address \glspl{VDM} as they are designed to extract specific content rather than compare the visual similarity between different documents.

While existing datasets have contributed to document understanding research, they primarily support tasks related to text extraction, classification, and structured information retrieval. These datasets do not provide a framework for evaluating \glspl{VDM}, as their organization is often driven by textual or semantic content rather than visual arrangement.

\section{Document Processing}

In the domain of document analysis, various studies have addressed challenges related to document layout analysis, classification with limited data, and similarity detection. Understanding these works provides valuable insights into the current landscape and highlights the unique contributions of the research.

Veneri et al.~\cite{veneri2022document} propose a method for Document Layout Analysis using variational autoencoders to detect deviations from a standard document template. Their approach is particularly suited for industrial compliance verification, where identifying visual discrepancies such as stamps, handwritten annotations, and misplaced signatures is crucial. By learning the distribution of compliant documents, the model detects anomalies as out-of-distribution samples, making it effective for scenarios with highly imbalanced datasets. While this study shares similarities with the work in identifying visual differences across documents, it is focused on anomaly detection within a predefined template rather than assessing layout similarity between different document classes.% Furthermore, their experiments were conducted on a private dataset, tailored for a specific industrial setting.

Zeghidi et al.~\cite{zeghidi2023cdpsim} present CDP-Sim, a similarity metric learning approach designed to detect counterfeit \glspl{CDP} using a Siamese neural network. This method effectively distinguishes original from fake \glspl{CDP} by learning a similarity metric that captures subtle differences between patterns. While their work is specifically applied to counterfeit detection, the concept of learning similarity metrics through metric learning and Siamese networks is broadly applicable to various tasks requiring fine-grained visual differentiation. This principle can also be leveraged in scenarios involving document layout comparisons, where structural relationships between documents must be assessed independently of their textual content.

Sinha et al.~\cite{sinha2024cica} introduce CICA, a framework that enhances CLIP's performance in zero-shot classification by improving textual-visual feature alignment through content-injected contrastive learning. This study emphasize data-efficient approaches to document classification. However, their approach is not suitable for a from-scratch training, and relies on costly pretrained models, limiting the potential to create a zero-shot cost-efficient model.

\section{LLMs on Document Understanding}

\glspl{LLM} have recently gained significant attention for their capabilities in understanding and generating human-like text. Their application in document understanding as a whole has shown promising results, particularly in tasks that require comprehension of complex layouts and multimodal information.

Scius et. al.~\cite{scius2024zeroshot} investigate the application of \glspl{LLM}, such as GPT-4 and RoBERTa, for zero-shot prompting and few-shot fine-tuning in document image classification. Their study demonstrates that \glspl{LLM} can achieve competitive performance with minimal labeled data, challenging the traditional reliance on large annotated datasets.

With the need to process multimodal information, there has been a rapid advancement in models with visual capabilities. Llama 3.2 Vision ~\cite{meta2024llama3.2}, for example, was introduced as an open source alternative with multimodal reasoning capabilities. Other recent work has analyzed the relationship between performance and efficiency. In 2024, DeepSeek-VL2~\cite{DBLP:journals/corr/abs-2412-10302} exemplified this trend, achieving competitive results with fewer activated parameters compared to models such as InternVL2 and Qwen2-VL.

In 2025, InternVL 2.5 was introduced as an evolution of its predecessor, InternVL 2.0, incorporating enhancements that resulted in performance gains, setting a new standard for open source multimodal models~\cite{internvl2024}. Moreover, its performance is competitive with leading proprietary models, such as OpenAI’s GPT-4o\cite{openai2024gpt4o,openai2024gpt4omini}.
In 2025, Qwen2.5-VL was introduced as an evolution of the Qwen2-VL series, surpassing its predecessor. According to its evaluation~\cite{bai2025qwen2}, Qwen2.5-VL excels in OCR-related tasks, chart interpretation, and document understanding. The leadership in state-of-the-art performance for these benchmarks is now primarily held by GPT-4o, InternVL 2.5, and Qwen2.5-VL, the latter two representing the strongest open-source alternatives.
